{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agrawalshubham665/airbyte-ai-chatbot/blob/main/airbyte_ai_chatbot_lms_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "Nmn7vi8ryZrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-core openai supabase"
      ],
      "metadata": {
        "id": "SgUJUFq2wJuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add Imports\n",
        "Note: Instead of using OpenAI to create the text embeddings for the user's question, we'll utilize Langchain's Fake Embedding model to save on cost for this demo."
      ],
      "metadata": {
        "id": "UbuXhGTMyS6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNO31YMYt78N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain_core.embeddings import DeterministicFakeEmbedding\n",
        "from supabase import create_client, Client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment Variables"
      ],
      "metadata": {
        "id": "cC3n80YtyjkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although we don't explicity call for your OpenAI API key here, we will still need to export the key to the environment to make it available"
      ],
      "metadata": {
        "id": "ylTGT1JNytVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "4KDTWC6SzDp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url: str = userdata.get('SUPABASE_URL')\n",
        "key: str = userdata.get('SUPABASE_KEY')\n",
        "api_key: str = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key = api_key\n"
      ],
      "metadata": {
        "id": "v9Ad-y2HyNhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07708733-4dc9-4d6a-e45f-9649dcfb617d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "Here we setup the connection to our Supabase instance and setup the use of the Fake Embedding model\n",
        "\n",
        "Note: The dimension size for the embedding model **must** match the chunk size we used for the destination on Airbyte"
      ],
      "metadata": {
        "id": "rdST-gKQ5jDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supabase: Client = create_client(url, key)\n",
        "\n",
        "embeddings = DeterministicFakeEmbedding(size=1536)"
      ],
      "metadata": {
        "id": "ZFXoJ_Qj0dFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Context based on user query\n",
        "Here we are calling the Postgres functions we created depending on what the user is querying for. Subsequently, this function will run a similarity search within the database to find results that answers the user's question"
      ],
      "metadata": {
        "id": "NWlwODfPBGdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(question: str) -> str:\n",
        "    question_embedding = embeddings.embed_query(question)\n",
        "    results = []\n",
        "    # Determine which table to query based on keywords in the question\n",
        "    if \"customer\" in question.lower():\n",
        "        query = supabase.rpc(\"find_related_customer\", {'question_vector': question_embedding}).execute()\n",
        "    elif \"product\" in question.lower():\n",
        "        query = supabase.rpc(\"find_related_products\", {'question_vector': question_embedding}).execute()\n",
        "    elif \"invoice\" in question.lower():\n",
        "        query = supabase.rpc(\"find_related_invoices\", {'question_vector': question_embedding}).execute()\n",
        "    else:\n",
        "        return \"No relevant context found for the given question.\"\n",
        "\n",
        "    # Process query results\n",
        "    for item in query.data:\n",
        "        results.append(item)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "7k4AAPlR015i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the response\n",
        "After we've created the context, we'll call the Chat Completions API from OpenAI to generate a proper response based on the context that we give it and the question we recieved from the user"
      ],
      "metadata": {
        "id": "-V6dWQutCUrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(question: str):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about the customers, products, and invoices provided to you in the context. Use only the provided context to answer questions. If the information isn't in the context, say so.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContext:\\n{get_context(question)}\"}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "p3IPFjGe1AB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask a question\n",
        "We are querying the sample data loaded originally into Stripe. That script generates dummy data. If your attempt below returns no data, double check the customer name exists. :)"
      ],
      "metadata": {
        "id": "2aa62gmNCmEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"Is there a customer named Justin? If so, show me his information\"\n",
        "answer = get_response(question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkOdkbPx1tWW",
        "outputId": "8f4773b4-ec3c-4c57-bb36-5d4321d14c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Yes, there is a customer named Justin. Here is his information:\n",
            "\n",
            "- **Name**: Justin Moore\n",
            "- **Email**: justin.moore@example.com\n",
            "- **Balance**: 0\n",
            "- **Created**: Timestamp 1738121053\n",
            "- **Delinquent**: False\n",
            "- **Description**: Sample customer for testing\n",
            "- **Invoice Prefix**: E2A8901D\n",
            "- **Next Invoice Sequence**: 2\n",
            "- **Preferred Locales**: []\n",
            "- **Tax Exempt**: none\n",
            "- **Live Mode**: False\n"
          ]
        }
      ]
    }
  ]
}